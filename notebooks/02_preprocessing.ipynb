{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca0e7c35-b163-41cf-8f8a-249bb54dd748",
   "metadata": {},
   "source": [
    "# 🧼 Breast Cancer Diagnosis – Data Preprocessing and Feature Preparation\n",
    "\n",
    "This notebook focuses on preparing the dataset for model training. It includes:\n",
    "\n",
    "- Cleaning and removing non-informative columns\n",
    "- Converting categorical labels into numerical format\n",
    "- Feature scaling\n",
    "- Splitting the dataset into training and test sets\n",
    "\n",
    "Proper preprocessing ensures that machine learning algorithms can learn effectively from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e3a75d-1335-4c17-8710-15f7da7644bd",
   "metadata": {},
   "source": [
    "## 🧭 Table of Contents\n",
    "\n",
    "1. [Objectives of this Notebook](#objectives-of-this-notebook)  \n",
    "2. [Column Removal](#column-removal)  \n",
    "3. [Label Encoding](#label-encoding)  \n",
    "4. [Feature Scaling](#feature-scaling)  \n",
    "5. [Train-Test Split](#train-test-split)  \n",
    "6. [Exporting Processed Data](#exporting-processed-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16562634-2239-4263-9e6b-fe6e46858dc6",
   "metadata": {},
   "source": [
    "## 1. Objectives of this Notebook <a id=\"objectives-of-this-notebook\"></a>\n",
    "\n",
    "In this notebook, we will prepare the dataset for supervised learning by performing the following steps:\n",
    "\n",
    "- **Column Removal**: Drop irrelevant or non-predictive columns (e.g., `id`, `Unnamed: 32`)\n",
    "- **Label Encoding**: Convert the target variable `diagnosis` into binary format\n",
    "- **Feature Scaling**: Standardize numerical features to improve model performance\n",
    "- **Train-Test Split**: Divide the dataset into training and test sets (e.g., 80/20 split)\n",
    "- **Exporting**: Save the processed datasets for modeling in the next phase\n",
    "> This notebook prepares the dataset for machine learning by ensuring clean, scaled, and encoded features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ccbde-d2b4-4373-aae0-45a992b0af00",
   "metadata": {},
   "source": [
    "## 2. Column Removal <a id=\"column-removal\"></a>\n",
    "\n",
    "Before modeling, we must remove columns that do not provide predictive value or are irrelevant for training.\n",
    "\n",
    "The following columns will be dropped:\n",
    "\n",
    "- `id`: A sample identifier with no clinical meaning or predictive power.\n",
    "- `Unnamed: 32`: An empty column included due to formatting in the original CSV file.\n",
    "\n",
    "Removing these ensures that only meaningful variables remain for preprocessing and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a22c8f-0330-4600-bad9-548998750ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785e0db2-00e5-4d1d-bacc-273eb947835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: 31\n",
      "['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/raw/data.csv')\n",
    "\n",
    "# Create a copy to avoid modifying the original DataFrame\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Create a copy to avoid modifying the original DataFrame\n",
    "df_cleaned = df_cleaned.drop(['id','Unnamed: 32'], axis=1)\n",
    "# (Previously identified as irrelevant in the EDA notebook)\n",
    "\n",
    "# Check resulting shape and remaining columns\n",
    "print(f\"Remaining columns: {df_cleaned.shape[1]}\")\n",
    "print(df_cleaned.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20c018-ee51-4f70-a745-ea875b7b881f",
   "metadata": {},
   "source": [
    "## 3. Label Encoding <a id=\"label-encoding\"></a>\n",
    "\n",
    "Now that irrelevant columns have been removed, we will encode the target variable for binary classification.\n",
    "\n",
    "The `diagnosis` column contains two categorical values:\n",
    "\n",
    "- `M` – Malignant tumor\n",
    "- `B` – Benign tumor\n",
    "\n",
    "These will be converted to numeric format:\n",
    "\n",
    "- `M` → `1`\n",
    "- `B` → `0`\n",
    "\n",
    "This transformation allows machine learning models to interpret the target variable as a binary classification task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea79a579-3b4d-4909-a940-398247d8fc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LabelEncoder mapping: {'B': 0, 'M': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode diagnosis using sklearn's LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_cleaned['target'] = le.fit_transform(df_cleaned['diagnosis'])\n",
    "\n",
    "# Confirm encoding\n",
    "print(\"LabelEncoder mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51904cb6-491a-4dd3-96b3-2755eb89b229",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling <a id=\"feature-scaling\"></a>\n",
    "\n",
    "Most machine learning models perform better when input features are on a similar scale. Since the variables in this dataset vary significantly in range (e.g., `area_worst` vs. `smoothness_mean`), feature scaling is essential.\n",
    "\n",
    "In this step, we will:\n",
    "\n",
    "- Select only the numeric predictor features (excluding `diagnosis` and `target`)\n",
    "- Apply **standardization** using `StandardScaler`:\n",
    "  - Subtracts the mean and scales to unit variance\n",
    "- Store the scaled features in a new DataFrame for training\n",
    "\n",
    "Standardization helps ensure that each feature contributes equally to the model and avoids bias toward larger-scale variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceceb9c5-8acc-47e5-9a62-7f44518fda81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean after scaling:\n",
      " radius_mean               -1.373633e-16\n",
      "texture_mean               6.868164e-17\n",
      "perimeter_mean            -1.248757e-16\n",
      "area_mean                 -2.185325e-16\n",
      "smoothness_mean           -8.366672e-16\n",
      "compactness_mean           1.873136e-16\n",
      "concavity_mean             4.995028e-17\n",
      "concave points_mean       -4.995028e-17\n",
      "symmetry_mean              1.748260e-16\n",
      "fractal_dimension_mean     4.745277e-16\n",
      "radius_se                  2.372638e-16\n",
      "texture_se                -1.123881e-16\n",
      "perimeter_se              -1.123881e-16\n",
      "area_se                   -1.311195e-16\n",
      "smoothness_se             -1.529727e-16\n",
      "compactness_se             1.748260e-16\n",
      "concavity_se               1.623384e-16\n",
      "concave points_se          0.000000e+00\n",
      "symmetry_se                8.741299e-17\n",
      "fractal_dimension_se      -6.243785e-18\n",
      "radius_worst              -8.241796e-16\n",
      "texture_worst              1.248757e-17\n",
      "perimeter_worst           -3.746271e-16\n",
      "area_worst                 0.000000e+00\n",
      "smoothness_worst          -2.372638e-16\n",
      "compactness_worst         -3.371644e-16\n",
      "concavity_worst            7.492542e-17\n",
      "concave points_worst       2.247763e-16\n",
      "symmetry_worst             2.622390e-16\n",
      "fractal_dimension_worst   -5.744282e-16\n",
      "dtype: float64\n",
      "Standard deviation after scaling:\n",
      " radius_mean                1.0\n",
      "texture_mean               1.0\n",
      "perimeter_mean             1.0\n",
      "area_mean                  1.0\n",
      "smoothness_mean            1.0\n",
      "compactness_mean           1.0\n",
      "concavity_mean             1.0\n",
      "concave points_mean        1.0\n",
      "symmetry_mean              1.0\n",
      "fractal_dimension_mean     1.0\n",
      "radius_se                  1.0\n",
      "texture_se                 1.0\n",
      "perimeter_se               1.0\n",
      "area_se                    1.0\n",
      "smoothness_se              1.0\n",
      "compactness_se             1.0\n",
      "concavity_se               1.0\n",
      "concave points_se          1.0\n",
      "symmetry_se                1.0\n",
      "fractal_dimension_se       1.0\n",
      "radius_worst               1.0\n",
      "texture_worst              1.0\n",
      "perimeter_worst            1.0\n",
      "area_worst                 1.0\n",
      "smoothness_worst           1.0\n",
      "compactness_worst          1.0\n",
      "concavity_worst            1.0\n",
      "concave points_worst       1.0\n",
      "symmetry_worst             1.0\n",
      "fractal_dimension_worst    1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "df_cleaned['target'] = le.fit_transform(df_cleaned['diagnosis'])\n",
    "\n",
    "# Select features\n",
    "X = df_cleaned.drop(columns=[\"diagnosis\", \"target\"])\n",
    "y = df_cleaned[\"target\"]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled_array = scaler.fit_transform(X)\n",
    "X_scaled = pd.DataFrame(X_scaled_array, columns=X.columns)\n",
    "\n",
    "# Save fitted scaler\n",
    "joblib.dump(scaler, \"../outputs/scaler.joblib\")\n",
    "\n",
    "# Check scaling\n",
    "print(\"Mean after scaling:\\n\", X_scaled.mean())\n",
    "print(\"Standard deviation after scaling:\\n\", X_scaled.std(ddof=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fa6c47-17a6-4078-bf37-e4bc67316041",
   "metadata": {},
   "source": [
    "> 💾 The fitted `StandardScaler` is saved using `joblib` for future use (e.g., during inference or deployment).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a0ec1d-b14f-46dc-a8ed-b3e8ee319286",
   "metadata": {},
   "source": [
    "### ⚙️ Feature Scaling with Standardization\n",
    "\n",
    "Feature scaling is an essential preprocessing step for many machine learning algorithms. Since the variables in this dataset differ significantly in magnitude (e.g., `area_worst` can be in the thousands while `smoothness_mean` is typically < 0.2), unscaled data can bias the model toward features with larger numeric values.\n",
    "\n",
    "In this notebook, we applied **standardization** using `StandardScaler`, which transforms each feature according to the following formula:\n",
    "\n",
    "$$\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- (x) is the original value\n",
    "- (mu) is the mean of the feature\n",
    "- (sigma) is the standard deviation of the feature\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 Expected Outcomes\n",
    "\n",
    "After scaling:\n",
    "- Each feature should have a **mean close to 0**\n",
    "- Each feature should have a **standard deviation close to 1**\n",
    "- The original shape and distribution of the data are preserved\n",
    "\n",
    "This transformation ensures that all features contribute equally to model training and avoids dominance by high-magnitude variables.\n",
    "\n",
    "To confirm the process, we printed the mean and standard deviation of all features post-scaling and observed values extremely close to 0 and exactly 1, respectively — indicating successful standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c061046a-13de-41b8-a4f4-129a1e0603b3",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split <a id=\"train-test-split\"></a>\n",
    "\n",
    "To assess how well our model generalizes to new, unseen data, we must divide the dataset into two separate subsets:\n",
    "\n",
    "- **Training set**: Used to train the model\n",
    "- **Test set**: Used to evaluate model performance on unseen data\n",
    "\n",
    "We will use an **80/20 split** — allocating 80% of the samples for training and 20% for testing. This is a widely accepted default that balances training robustness with evaluation reliability.\n",
    "\n",
    "To ensure reproducibility of our results, we will set a fixed `random_state`.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ What we will do:\n",
    "\n",
    "- Use `train_test_split()` from `sklearn.model_selection`\n",
    "- Input: preprocessed feature matrix `X_scaled` and target vector `y`\n",
    "- Parameters:\n",
    "  - `test_size=0.2`\n",
    "  - `random_state=42`\n",
    "  - `stratify=y` to preserve the class distribution in both sets\n",
    "\n",
    "After splitting, we will print the shape of each subset to confirm the correct partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a5fab14-4665-4e5d-9b63-8a5f5649c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b492fdc-57d1-4540-a9d7-6aba18e9bef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (455, 30)\n",
      "X_test shape:  (114, 30)\n",
      "y_train shape: (455,)\n",
      "y_test shape:  (114,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and test sets\n",
    "# 80% of the data will be used to train the model, 20% to evaluate it\n",
    "# We use stratify=y to preserve the same class balance in both sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled,  # Scaled features\n",
    "    y,         # Target variable\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Print the shape of the resulting sets to verify the split\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfd8b9-4f9c-42fa-a2a2-2826669acef7",
   "metadata": {},
   "source": [
    "We split the dataset using an 80/20 ratio, where 80% of the samples are used for training and 20% for testing. This produced the following shapes:\n",
    "\n",
    "- `X_train`: shape (455, 30) — Scaled features for training\n",
    "- `X_test`: shape (114, 30) — Scaled features for testing\n",
    "- `y_train`: shape (455,) — Corresponding target labels for training\n",
    "- `y_test`: shape (114,) — Corresponding target labels for testing\n",
    "\n",
    "The `train_test_split()` function includes several key parameters:\n",
    "\n",
    "- `test_size=0.2`: Reserves 20% of the data for the test set\n",
    "- `random_state=42`: Ensures reproducibility of the split\n",
    "- `stratify=y`: Maintains the original class distribution in both training and test sets\n",
    "\n",
    "Using `stratify=y` is essential in medical classification problems, where class imbalance (e.g., benign vs. malignant tumors) can lead to misleading performance metrics if not properly accounted for. This guarantees that both subsets are statistically representative of the overall diagnosis distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37afb67f-c2b1-4310-8ae5-c25b32952877",
   "metadata": {},
   "source": [
    "## 6. Exporting Processed Data <a id=\"exporting-processed-data\"></a>\n",
    "\n",
    "After cleaning, encoding, scaling, and splitting the dataset, we now save the processed data to disk. This allows us to reuse these subsets directly in the modeling phase without repeating the preprocessing steps.\n",
    "\n",
    "We will export the following components as CSV files:\n",
    "\n",
    "- `X_train.csv` and `X_test.csv`: Feature matrices\n",
    "- `y_train.csv` and `y_test.csv`: Target vectors\n",
    "\n",
    "All files will be saved under the `data/processed/` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea3589b-504a-467f-aeb4-7d3479d93c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed data successfully exported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "# Save the processed training and test sets\n",
    "X_train.to_csv(\"../data/processed/X_train.csv\", float_format=\"%.6f\", index=False)\n",
    "X_test.to_csv(\"../data/processed/X_test.csv\", float_format=\"%.6f\", index=False)\n",
    "y_train.to_csv(\"../data/processed/y_train.csv\", index=False)\n",
    "y_test.to_csv(\"../data/processed/y_test.csv\", index=False)\n",
    "\n",
    "print(\"✅ Processed data successfully exported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af5330d-630c-4d97-a5e0-df1b09ff4c64",
   "metadata": {},
   "source": [
    "## 📌 Summary and Next Steps <a id=\"summary-and-next-steps\"></a>\n",
    "\n",
    "In this notebook, we prepared the Breast Cancer Wisconsin dataset for machine learning by performing the following steps:\n",
    "\n",
    "- 🧹 Removed irrelevant columns (`id`, `Unnamed: 32`)\n",
    "- 🏷️ Encoded the `diagnosis` variable into a binary `target` column\n",
    "- 📏 Scaled all numerical features using `StandardScaler`\n",
    "- 🔀 Split the dataset into training and test sets (80/20), preserving class balance with `stratify=y`\n",
    "- 💾 Exported all processed data subsets for reuse in future modeling\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "In the next notebook, we will:\n",
    "\n",
    "- Load the processed data\n",
    "- Train and evaluate several classification models (e.g., Logistic Regression, Random Forest, etc.)\n",
    "- Compare their performance using cross-validation and metrics such as accuracy, precision, recall, and ROC-AUC\n",
    "\n",
    "We are now ready to begin the modeling phase of the project.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
